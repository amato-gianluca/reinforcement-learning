{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import numpy as np\n",
    "from itertools import accumulate\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ActionValue.multi_armed_bandit import MultiArmedBandit\n",
    "from ActionValue.action_value import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbed_env_gen = lambda: MultiArmedBandit.random_gen(10, scale = 1)\n",
    "default_numsteps = 1000\n",
    "default_numruns = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_env_gen = lambda: MultiArmedBandit([0.2, -0.9, 1.5, 0.4, 1.2, -1.5, -0.2, -1.1, 0.8, -0.5 ], scale = 1)\n",
    "plt.violinplot(np.random.randn(200, 10) + book_env_gen().means)\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Reward distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(avm, numsteps):\n",
    "    for i in range(numsteps):\n",
    "        avm.step()\n",
    "    cumulative_mean_rewards = [ r/n for r, n in zip(list(accumulate(avm.env.rewards)), range(1,numsteps+1)) ]\n",
    "    cumulative_best_action_perc = [ r/n for r, n in zip(list(accumulate(avm.env.best_actions)), range(1,numsteps+1)) ]\n",
    "    return avm.env.rewards.copy(), cumulative_mean_rewards, cumulative_best_action_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = MeanValueGreedy(book_env_gen())\n",
    "g2 = MeanValueEpsilonGreedy(book_env_gen(), epsilon = 0.01)\n",
    "g3 = MeanValueEpsilonGreedy(book_env_gen(), epsilon = 0.1)\n",
    "rewards1, mean_rewards1, best_actions_perc1 = single_run(g1, default_numsteps)\n",
    "rewards2, mean_rewards2, best_actions_perc2 = single_run(g2, default_numsteps)\n",
    "rewards3, mean_rewards3, best_actions_perc3 = single_run(g3, default_numsteps)\n",
    "plt.plot(range(default_numsteps), mean_rewards1, label = \"greedy\")\n",
    "plt.plot(range(default_numsteps), mean_rewards2, label = \"0.01-greedy\")\n",
    "plt.plot(range(default_numsteps), mean_rewards3, label = \"0.1-greedy\")\n",
    "plt.title(\"single run execution on the testbed\")\n",
    "plt.xlabel(\"iteration step\")\n",
    "plt.ylabel(\"cumulative mean of rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_run(avm_gen, numsteps, numruns):\n",
    "    mean_rewards = np.zeros(numsteps)\n",
    "    mean_bestactions = np.zeros(numsteps)\n",
    "    for r in range(numruns):\n",
    "        avm = avm_gen()\n",
    "        for i in range(numsteps):\n",
    "            avm.step()\n",
    "        mean_rewards += avm.env.rewards\n",
    "        mean_bestactions += avm.env.best_actions\n",
    "    mean_rewards /= numruns\n",
    "    mean_bestactions /= numruns\n",
    "    return mean_rewards, mean_bestactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_run_graph(labels, mean_rewards, mean_bestactions, title=\"\"):\n",
    "    plots = len(labels)\n",
    "    steps = len(mean_rewards[0])\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(121)\n",
    "    for i in range(plots):\n",
    "        plt.plot(range(steps), mean_rewards[i], label = labels[i])\n",
    "    plt.xlabel(\"iteraton steps\")\n",
    "    plt.ylabel(\"mean reward\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    for i in range(plots):\n",
    "        plt.plot(range(steps), mean_bestactions[i], label = labels[i])\n",
    "    plt.xlabel(\"iteraton steps\")\n",
    "    plt.ylabel(\"% of correct actions\")\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = multi_run(lambda: MeanValueEpsilonGreedy(testbed_env_gen(), epsilon = 0), default_numsteps, default_numruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards1, mean_bestactions1 = multi_run(\n",
    "    lambda: MeanValueEpsilonGreedy(testbed_env_gen(), epsilon = 0), default_numsteps, default_numruns)\n",
    "mean_rewards2, mean_bestactions2 = multi_run(\n",
    "    lambda: MeanValueEpsilonGreedy(testbed_env_gen(), epsilon = 0.01), default_numsteps, default_numruns)\n",
    "mean_rewards3, mean_bestactions3 = multi_run(\n",
    "    lambda: MeanValueEpsilonGreedy(testbed_env_gen(), epsilon = 0.1), default_numsteps, default_numruns)\n",
    "multi_run_graph([\"greedy\", \"0.01-greey\", \"0.1-greedy\"],\n",
    "                [mean_rewards1, mean_rewards2, mean_rewards3],\n",
    "                [mean_bestactions1, mean_bestactions2, mean_bestactions3],\n",
    "                title=\"Mean-value algorithms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards1, mean_bestactions1 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.01), default_numsteps, default_numruns)\n",
    "mean_rewards2, mean_bestactions2 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.05), default_numsteps, default_numruns)\n",
    "mean_rewards3, mean_bestactions3 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.1), default_numsteps, default_numruns)\n",
    "mean_rewards4, mean_bestactions4 = multi_run(\n",
    "    lambda: MeanValueEpsilonGreedy(testbed_env_gen(), epsilon = 0.1), default_numsteps, default_numruns)\n",
    "multi_run_graph([\"alpha = 0.01\", \"alpha = 0.05\", \"alpha = 0.1\", \"mean-value\"],\n",
    "                [mean_rewards1, mean_rewards2, mean_rewards3, mean_rewards4],\n",
    "                [mean_bestactions1, mean_bestactions2, mean_bestactions3, mean_bestactions4],\n",
    "                title = \"Constant-step and mean-value 0.1-greedy algorithms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards1, mean_bestactions1 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.0, alpha = 0.1, initial_preference = 5 ),\n",
    "    default_numsteps, default_numruns)\n",
    "mean_rewards2, mean_bestactions2 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.0, alpha = 0.1),\n",
    "    default_numsteps, default_numruns)\n",
    "mean_rewards3, mean_bestactions3 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.1),\n",
    "    default_numsteps, default_numruns)\n",
    "multi_run_graph([\"alpha=0.1 greedy Q_init=5\", \"alpha=0.1 greedy Q_init=0\",\"alpha=0.1 0.1-greedy Q_init=0\"],\n",
    "                [mean_rewards1, mean_rewards2, mean_rewards3],\n",
    "                [mean_bestactions1, mean_bestactions2, mean_bestactions3],\n",
    "                title=\"Algorithms with different initial estimates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonstationary_env_gen = lambda: MultiArmedBandit([0.0] * 10, scale = 1, drift = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = multi_run(lambda: MeanValueEpsilonGreedy(nonstationary_env_gen(), epsilon = 0), default_numsteps, default_numruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards1, mean_bestactions1 = multi_run(\n",
    "    lambda: MeanValueEpsilonGreedy(nonstationary_env_gen(), epsilon = 0.1), 10000, default_numruns)\n",
    "mean_rewards2, mean_bestactions2 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(nonstationary_env_gen(), epsilon = 0.1, alpha = 0.1), 10000, default_numruns)\n",
    "multi_run_graph([\"mean-value, 0.1-greedy\", \"alpha = 0.1, 0.1-greedy\"],\n",
    "                [mean_rewards1, mean_rewards2],\n",
    "                [mean_bestactions1, mean_bestactions2],\n",
    "                title = \"Nonstationary environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards1, mean_bestactions1 = multi_run(\n",
    "    lambda: MeanValueUCB(testbed_env_gen(), c = 2), default_numsteps, default_numruns)\n",
    "mean_rewards2, mean_bestactions2 = multi_run(\n",
    "    lambda: MeanValueEpsilonGreedy(testbed_env_gen(), epsilon = 0.1), default_numsteps, default_numruns)\n",
    "multi_run_graph([\"nean-value UCB c=2\", \"mean-value 0.1-greedy\"],\n",
    "                [mean_rewards1, mean_rewards2],\n",
    "                [mean_bestactions1, mean_bestactions2],\n",
    "                title=\"UCB vs epsilon greedy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbed_baseline_env_gen = lambda: MultiArmedBandit.random_gen(10, mean_loc=4.0, scale = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = multi_run(lambda: GradientAlgorithm(testbed_baseline_env_gen(), alpha=0.1), default_numsteps, default_numruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards1, mean_bestactions1 = multi_run(\n",
    "    lambda: GradientAlgorithm(testbed_baseline_env_gen(), alpha=0.1), default_numsteps, default_numruns)\n",
    "mean_rewards2, mean_bestactions2 = multi_run(\n",
    "    lambda: GradientAlgorithm(testbed_baseline_env_gen(), alpha=0.4), default_numsteps, default_numruns)\n",
    "mean_rewards3, mean_bestactions3 = multi_run(\n",
    "    lambda: GradientAlgorithm(testbed_baseline_env_gen(), alpha=0.1, baseline=False), default_numsteps, default_numruns)\n",
    "mean_rewards4, mean_bestactions4 = multi_run(\n",
    "    lambda: GradientAlgorithm(testbed_baseline_env_gen(), alpha=0.4, baseline=False), default_numsteps, default_numruns)\n",
    "multi_run_graph([\"baseline alpha=0.1\", \"baseline alpha=0.4\",\"no baseline alpha=0.1\", \"no baseline alpha=0.4\" ],\n",
    "                [mean_rewards1, mean_rewards2, mean_rewards3, mean_rewards4],\n",
    "                [mean_bestactions1, mean_bestactions2, mean_bestactions3, mean_bestactions4],\n",
    "                title=\"Gradient based algorithm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards1, mean_bestactions1 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.1), default_numsteps, default_numruns)\n",
    "mean_rewards2, mean_bestactions2 = multi_run(\n",
    "    lambda: ExpWeightNoBiasEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.1),\n",
    "    default_numsteps, default_numruns)\n",
    "mean_rewards3, mean_bestactions3 = multi_run(\n",
    "    lambda: ConstantStepEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.01), default_numsteps, default_numruns)\n",
    "mean_rewards4, mean_bestactions4 = multi_run(\n",
    "    lambda: ExpWeightNoBiasEpsilonGreedy(testbed_env_gen(), epsilon = 0.1, alpha = 0.01), default_numsteps, default_numruns)\n",
    "multi_run_graph([\"alpha = 0.1\", \"alpha = 0.1 nobias\", \"alpha = 0.01\", \"alpha = 0.01 nobias\"],\n",
    "                [mean_rewards1, mean_rewards2, mean_rewards3, mean_rewards4],\n",
    "                [mean_bestactions1, mean_bestactions2, mean_bestactions3, mean_bestactions4],\n",
    "                title = \"Constant-step 0.1-greedy algorithms with and without bias\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}